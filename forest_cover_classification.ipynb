{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codecademy portfolio project - Forest Cover Classification\n",
    "\n",
    "In this notebook I present my solution to the final portfolio project of Codecademy's intro-level course to deep learning and tensorflow. Data was provided from codecademy on geological aspects of the areas studied (obtained from the US Geological Survey and US Forest Service (USFS)), one observation being a 30x30 meter cell of forest. Labelling was determined from the USFS Region 2 Resource Information System data. \n",
    "\n",
    "Covertypes: \n",
    "- Spruce/Fir\n",
    "- Lodgepole Pine\n",
    "- Ponderosa Pine\n",
    "- Cottonwood/Willow\n",
    "- Aspen\n",
    "- Douglas-fir\n",
    "- Krummholz\n",
    "\n",
    "581012 observations were provided with 54 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cover_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_balance = data['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(height=class_balance.values, x=class_balance.index, color='teal', edgecolor='black')\n",
    "plt.title('Class Imbalance', size=18)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Percentage of all classes in the Dataset')\n",
    "plt.savefig('class_imbalance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns_pairplot_1 = sns.pairplot(data[list(data.columns[:10]) + ['class']].sample(1000), hue='class', palette='colorblind', plot_kws={'alpha': .5})\n",
    "sns_pairplot_1.savefig('pairplot_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting all wilderness area type by forest cover type \n",
    "for i in range(4):\n",
    "    countplot_wilderness = plt.figure()\n",
    "    sns.countplot(x=f'Wilderness_Area{i+1}', hue='class', data=data[list(data.columns[10:14]) + ['class']], palette='colorblind')\n",
    "    countplot_wilderness.savefig(f'Wilderness_Area_countplot_{i+1}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all wilderness area type by forest cover type \n",
    "wilderness_class_tables = []\n",
    "for i in range(4):\n",
    "    wilderness_class_table = data.groupby('class').mean()[f'Wilderness_Area{i+1}']\n",
    "    wilderness_class_tables.append(wilderness_class_table)\n",
    "\n",
    "wilderness_class_df = pd.DataFrame(wilderness_class_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of wilderness area type by forest cover type\n",
    "wilderness_heatmap = plt.figure()\n",
    "g = sns.heatmap(wilderness_class_df, cmap='viridis', annot=True)\n",
    "g.axes.xaxis.set_ticks_position('top')\n",
    "g.xaxis.set_label_position('top') \n",
    "g.set_title('Proportion of Wilderness Area Types per Class', size=18)\n",
    "\n",
    "wilderness_heatmap.savefig('wilderness_heatmap.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all soil type by forest cover type \n",
    "soiltype_class_tables = []\n",
    "for i in range(40):\n",
    "    soiltype_class_table = data.groupby('class').mean()[f'Soil_Type{i+1}']\n",
    "    soiltype_class_tables.append(soiltype_class_table)\n",
    "\n",
    "soiltype_class_df = pd.DataFrame(soiltype_class_tables)\n",
    "\n",
    "# creating a heatmap of soil type per forest cover type\n",
    "soiltype_heatmap = plt.figure(figsize=(10,10))\n",
    "g = sns.heatmap(soiltype_class_df, cmap='viridis', annot=True)\n",
    "g.axes.xaxis.set_ticks_position(\"top\")\n",
    "g.xaxis.set_label_position('top') \n",
    "g.set_title('Proportion of Soil Types per Class', size=18)\n",
    "soiltype_heatmap.savefig('soiltype_heatmap.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Since categorical variables were already dummified, the only preprocessing steps necessary are feature scaling and label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate predictors from the target\n",
    "y = data['class']\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23, stratify=y)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "Since dealing with tabular data. A feed-forward model was chosen with Dense layers as hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instantiation\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# hidden layers\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(14, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(0.005), metrics=[tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.AUC()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=512, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = tf.keras.models.load_model('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the loss curve of train and validation set\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "ax1.set_title('model loss')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['train', 'validation'], loc=[1,0])\n",
    "fig.tight_layout()\n",
    "fig.savefig('loss_curve.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing loss, accurcy and auc for the testset\n",
    "loss, acc, auc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# collecting predictions\n",
    "y_estimate = model.predict(X_test)\n",
    "y_estimate = np.argmax(y_estimate, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the confusion matrix of the test set\n",
    "sns.set_style('white')\n",
    "fig3 = plt.figure(figsize=(10,10))\n",
    "ax1 = fig3.add_subplot()\n",
    "plt.title('Confusion Matrix', size=18)\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_estimate, normalize='true', values_format='.2f', ax=ax1)\n",
    "plt.savefig('conf_matrix_plot.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d58fcea9115c0c130995fd85f2b9b5d654e57f0a1c46b371c113db1ca1460439"
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
